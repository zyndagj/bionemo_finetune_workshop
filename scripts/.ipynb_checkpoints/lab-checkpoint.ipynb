{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5549b6-acee-4c42-82b7-0560ff59b19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0AVI4</td>\n",
       "      <td>MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Endoplasmic reticulum me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0JLT2</td>\n",
       "      <td>MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Nucleus {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6</td>\n",
       "      <td>GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0PJY2</td>\n",
       "      <td>MDSSCHNATTKMLATAPARGNMMSTSKPLAFSIERIMARTPEPKAL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Nucleus {ECO:0000269|Pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11972</th>\n",
       "      <td>Q9H8W2</td>\n",
       "      <td>MRPGSSPRAPECGAPALPRPQLDRLPARPAPSRGRGAPSLRWPAKE...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11973</th>\n",
       "      <td>Q9HAA7</td>\n",
       "      <td>MLFGIRILVNTPSPLVTGLHHYNPSIHRDQGECANQWRKGPGSAHL...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11974</th>\n",
       "      <td>Q9NZ38</td>\n",
       "      <td>MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>Q9UFV3</td>\n",
       "      <td>MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>Q9Y6C7</td>\n",
       "      <td>MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11977 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "1          A0AVI4  MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...   \n",
       "2          A0JLT2  MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...   \n",
       "3          A0M8Q6  GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...   \n",
       "4          A0PJY2  MDSSCHNATTKMLATAPARGNMMSTSKPLAFSIERIMARTPEPKAL...   \n",
       "...           ...                                                ...   \n",
       "11972      Q9H8W2  MRPGSSPRAPECGAPALPRPQLDRLPARPAPSRGRGAPSLRWPAKE...   \n",
       "11973      Q9HAA7  MLFGIRILVNTPSPLVTGLHHYNPSIHRDQGECANQWRKGPGSAHL...   \n",
       "11974      Q9NZ38  MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...   \n",
       "11975      Q9UFV3  MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...   \n",
       "11976      Q9Y6C7  MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "0      SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...  \n",
       "1      SUBCELLULAR LOCATION: Endoplasmic reticulum me...  \n",
       "2           SUBCELLULAR LOCATION: Nucleus {ECO:0000305}.  \n",
       "3      SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...  \n",
       "4      SUBCELLULAR LOCATION: Nucleus {ECO:0000269|Pub...  \n",
       "...                                                  ...  \n",
       "11972                                               None  \n",
       "11973                                               None  \n",
       "11974                                               None  \n",
       "11975                                               None  \n",
       "11976                                               None  \n",
       "\n",
       "[11977 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "import requests, pandas, os\n",
    "from io import BytesIO\n",
    "\n",
    "query_url = \"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Ccc_subcellular_location&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B80%20TO%20500%5D%29%29\"\n",
    "tmp_file = \"/tmp/uniprot.parquet.gz\"\n",
    "\n",
    "if not os.path.exists(tmp_file):\n",
    "    # Get data\n",
    "    uniprot_request = requests.get(query_url)\n",
    "    # Store data as binary object\n",
    "    bio = BytesIO(uniprot_request.content)\n",
    "    # Reach bionary object as compressed csv\n",
    "    df = pandas.read_csv(bio, compression='gzip', sep='\\t')\n",
    "    # Write out to local location for faster reloads\n",
    "    df.to_parquet(tmp_file, compression=\"gzip\")\n",
    "else:\n",
    "    # Load from checkpoint\n",
    "    df = pandas.read_parquet(tmp_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4ae7c9-4161-40c9-850d-6d5036f022fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A1E959</td>\n",
       "      <td>MKIIILLGFLGATLSAPLIPQRLMSASNSNELLLNLNNGQLLPLQL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000250|Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A1XBS5</td>\n",
       "      <td>MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A2RU49</td>\n",
       "      <td>MSSGNYQQSEALSKPTFSEEQASALVESVFGLKVSKVRPLPSYDDQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A2RUH7</td>\n",
       "      <td>MEAATAPEVAAGSKLKVKEASPADAEPPQASPGQGAGSPTPQLLPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, myofibril, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A4D126</td>\n",
       "      <td>MEAGPPGSARPAEPGPCLSGQRGADHTASASLQSVAGTEPGRHPQA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytosol {ECO:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>Q8NBC4</td>\n",
       "      <td>MFPRPVLNSRAQAILLPQPPNMLDHRQWPPRLASFPFTKTGMLSRA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11535</th>\n",
       "      <td>Q8TDY3</td>\n",
       "      <td>MFNPHALDSPAVIFDNGSGFCKAGLSGEFGPRHMVSSIVGHLKFQA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547</th>\n",
       "      <td>Q8WWF8</td>\n",
       "      <td>MAGTARHDREMAIQAKKKLTTATDPIERLRLQCLARGSAGIKGLGR...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11671</th>\n",
       "      <td>Q9NUJ7</td>\n",
       "      <td>MGGQVSASNSFSRLHCRNANEDWMSALCPRLWDVPLHHLSIPGSHD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11679</th>\n",
       "      <td>Q9P2W6</td>\n",
       "      <td>MGRTWCGMWRRRRPGRRSAVPRWPHLSSQSGVEPPDRWTGTPGWPS...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2644 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry                                           Sequence  \\\n",
       "9      A1E959  MKIIILLGFLGATLSAPLIPQRLMSASNSNELLLNLNNGQLLPLQL...   \n",
       "14     A1XBS5  MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...   \n",
       "18     A2RU49  MSSGNYQQSEALSKPTFSEEQASALVESVFGLKVSKVRPLPSYDDQ...   \n",
       "20     A2RUH7  MEAATAPEVAAGSKLKVKEASPADAEPPQASPGQGAGSPTPQLLPP...   \n",
       "21     A4D126  MEAGPPGSARPAEPGPCLSGQRGADHTASASLQSVAGTEPGRHPQA...   \n",
       "...       ...                                                ...   \n",
       "11495  Q8NBC4  MFPRPVLNSRAQAILLPQPPNMLDHRQWPPRLASFPFTKTGMLSRA...   \n",
       "11535  Q8TDY3  MFNPHALDSPAVIFDNGSGFCKAGLSGEFGPRHMVSSIVGHLKFQA...   \n",
       "11547  Q8WWF8  MAGTARHDREMAIQAKKKLTTATDPIERLRLQCLARGSAGIKGLGR...   \n",
       "11671  Q9NUJ7  MGGQVSASNSFSRLHCRNANEDWMSALCPRLWDVPLHHLSIPGSHD...   \n",
       "11679  Q9P2W6  MGRTWCGMWRRRRPGRRSAVPRWPHLSSQSGVEPPDRWTGTPGWPS...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "9      SUBCELLULAR LOCATION: Secreted {ECO:0000250|Un...  \n",
       "14     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "18        SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.  \n",
       "20     SUBCELLULAR LOCATION: Cytoplasm, myofibril, sa...  \n",
       "21     SUBCELLULAR LOCATION: Cytoplasm, cytosol {ECO:...  \n",
       "...                                                  ...  \n",
       "11495  SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "11535  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton ...  \n",
       "11547     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.  \n",
       "11671  SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "11679                   SUBCELLULAR LOCATION: Cytoplasm.  \n",
       "\n",
       "[2644 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()  # Drop proteins with missing columns\n",
    "cytosolic = df['Subcellular location [CC]'].str.contains(\"Cytosol\") | df['Subcellular location [CC]'].str.contains(\"Cytoplasm\")\n",
    "membrane = df['Subcellular location [CC]'].str.contains(\"Membrane\") | df['Subcellular location [CC]'].str.contains(\"Cell membrane\")\n",
    "cytosolic_df = df[cytosolic & ~membrane]\n",
    "cytosolic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7079889f-4942-420b-82cc-392b3de0f420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6</td>\n",
       "      <td>GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A2RU14</td>\n",
       "      <td>MAGTVLGVGAGVFILALLWVAVLLLCVLLSRASGAARFSVIFLFFG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A5X5Y0</td>\n",
       "      <td>MEGSWFHRKRFSFYLLLGFLLQGRGVTFTINCSGFGQHGADPTALN...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Postsynaptic cell membra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>A6ND01</td>\n",
       "      <td>MACWWPLLLELWTVMPTWAGDELLNICMNAKHHKRVPSPEDKLYEE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cell membrane {ECO:00002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11895</th>\n",
       "      <td>Q86UQ5</td>\n",
       "      <td>MQSDIYHPGHSFPSWVLCWVHSCGHEGHLRETAEIRKTHQNGDLQI...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11918</th>\n",
       "      <td>Q8N8V8</td>\n",
       "      <td>MLLKVRRASLKPPATPHQGAFRAGNVIGQLIYLLTWSLFTAWLRPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11955</th>\n",
       "      <td>Q96N68</td>\n",
       "      <td>MQGQGALKESHIHLPTEQPEASLVLQGQLAESSALGPKGALRPQAQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11963</th>\n",
       "      <td>Q9H0A3</td>\n",
       "      <td>MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVT...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11966</th>\n",
       "      <td>Q9H354</td>\n",
       "      <td>MNKHNLRLVQLASELILIEIIPKLFLSQVTTISHIKREKIPPNHRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2538 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "3          A0M8Q6  GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...   \n",
       "17         A2RU14  MAGTVLGVGAGVFILALLWVAVLLLCVLLSRASGAARFSVIFLFFG...   \n",
       "33         A5X5Y0  MEGSWFHRKRFSFYLLLGFLLQGRGVTFTINCSGFGQHGADPTALN...   \n",
       "36         A6ND01  MACWWPLLLELWTVMPTWAGDELLNICMNAKHHKRVPSPEDKLYEE...   \n",
       "...           ...                                                ...   \n",
       "11895      Q86UQ5  MQSDIYHPGHSFPSWVLCWVHSCGHEGHLRETAEIRKTHQNGDLQI...   \n",
       "11918      Q8N8V8  MLLKVRRASLKPPATPHQGAFRAGNVIGQLIYLLTWSLFTAWLRPP...   \n",
       "11955      Q96N68  MQGQGALKESHIHLPTEQPEASLVLQGQLAESSALGPKGALRPQAQ...   \n",
       "11963      Q9H0A3  MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVT...   \n",
       "11966      Q9H354  MNKHNLRLVQLASELILIEIIPKLFLSQVTTISHIKREKIPPNHRK...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "0      SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...  \n",
       "3      SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...  \n",
       "17     SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "33     SUBCELLULAR LOCATION: Postsynaptic cell membra...  \n",
       "36     SUBCELLULAR LOCATION: Cell membrane {ECO:00002...  \n",
       "...                                                  ...  \n",
       "11895  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11918  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11955  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11963  SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...  \n",
       "11966  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "\n",
       "[2538 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "membrane_df = df[membrane & ~cytosolic]\n",
    "membrane_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c00518a4-9750-47cf-aac6-f8789028a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cytosolic_sequences = cytosolic_df[\"Sequence\"].tolist()\n",
    "cytosolic_labels = [0 for protein in cytosolic_sequences]\n",
    "membrane_sequences = membrane_df[\"Sequence\"].tolist()\n",
    "membrane_labels = [1 for protein in membrane_sequences]\n",
    "\n",
    "sequences = cytosolic_sequences + membrane_sequences\n",
    "labels = cytosolic_labels + membrane_labels\n",
    "\n",
    "# Quick check to make sure we got it right\n",
    "assert(len(sequences) == len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9bdc9c-50eb-402b-97a5-3f4a7af4a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ae2a58-6801-4228-8536-b1b35440cdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df582064-246f-4343-a1a6-b77dedb0304d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 20, 5, 9, 7, 16, 7, 4, 7, 4, 13, 6, 10, 6, 21, 4, 4, 6, 10, 4, 5, 5, 12, 7, 5, 15, 16, 7, 4, 4, 6, 10, 15, 7, 7, 7, 7, 10, 23, 9, 6, 12, 17, 12, 8, 6, 17, 18, 19, 10, 17, 15, 4, 15, 19, 4, 5, 18, 4, 10, 15, 10, 20, 17, 11, 17, 14, 8, 10, 6, 14, 19, 21, 18, 10, 5, 14, 8, 10, 12, 18, 22, 10, 11, 7, 10, 6, 20, 4, 14, 21, 15, 11, 15, 10, 6, 16, 5, 5, 4, 13, 10, 4, 15, 7, 18, 13, 6, 12, 14, 14, 14, 19, 13, 15, 15, 15, 10, 20, 7, 7, 14, 5, 5, 4, 15, 7, 7, 10, 4, 15, 14, 11, 10, 15, 18, 5, 19, 4, 6, 10, 4, 5, 21, 9, 7, 6, 22, 15, 19, 16, 5, 7, 11, 5, 11, 4, 9, 9, 15, 10, 15, 9, 15, 5, 15, 12, 21, 19, 10, 15, 15, 15, 16, 4, 20, 10, 4, 10, 15, 16, 5, 9, 15, 17, 7, 9, 15, 15, 12, 13, 15, 19, 11, 9, 7, 4, 15, 11, 21, 6, 4, 4, 7, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2475b80-405a-4e88-927b-b0c79131fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "348deb2a-6cce-4e46-a537-4801974ee75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3886, 3) ['input_ids', 'attention_mask', 'labels']\n",
      "(1296, 3) ['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/usr/local/lib/python3.12/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "test_dataset\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "\n",
    "# Print the shape and columns of the datasets after adding labels\n",
    "print(train_dataset.shape, list(train_dataset[0].keys()))\n",
    "print(test_dataset.shape, list(test_dataset[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f92ae8-e17b-444f-83d7-b8921e0c8708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = max(train_labels + test_labels) + 1  # 2: {0: cytosolic, 1: membrane}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bff0dcbf-12b8-48b3-b900-dd6d4dece154",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "strat = \"steps\" # \"epoch\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-localization\",\n",
    "    eval_strategy = strat,\n",
    "    save_strategy = strat,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    max_steps=100,\n",
    "    eval_steps=20,\n",
    "    #num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    include_tokens_per_second=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "820334ca-af52-43c1-8b07-a1b3ee5c0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1b65d8f-0930-443f-9947-99a00eb83ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298/2227356937.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e795d1c-86d7-4968-b4d5-d58ff4739c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.317503</td>\n",
       "      <td>0.942130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.336070</td>\n",
       "      <td>0.939043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.344598</td>\n",
       "      <td>0.940586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.346892</td>\n",
       "      <td>0.943673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.346528</td>\n",
       "      <td>0.942901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at esm2_t33_650M_UR50D-finetuned-localization/checkpoint-80/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.013548849821090699, metrics={'train_runtime': 179.6321, 'train_samples_per_second': 4.454, 'train_steps_per_second': 0.557, 'train_tokens_per_second': 2097.62, 'total_flos': 1403037318561600.0, 'train_loss': 0.013548849821090699, 'epoch': 0.205761316872428})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e6b7b2f-fdb4-483f-bfdd-e30cebdb2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import Callback, RichModelSummary\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from megatron.core.optimizer.optimizer_config import OptimizerConfig\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm as nllm\n",
    "from nemo.lightning import resume\n",
    "from nemo.lightning.nemo_logger import NeMoLogger\n",
    "from nemo.lightning.pytorch import callbacks as nl_callbacks\n",
    "from nemo.lightning.pytorch.callbacks.model_transform import ModelTransform\n",
    "from nemo.lightning.pytorch.callbacks.peft import PEFT\n",
    "from nemo.lightning.pytorch.optim.megatron import MegatronOptimizerModule\n",
    "\n",
    "from bionemo.core.data.load import load\n",
    "from bionemo.esm2.api import ESM2GenericConfig\n",
    "from bionemo.esm2.data.tokenizer import BioNeMoESMTokenizer, get_tokenizer\n",
    "from bionemo.esm2.model.finetune.datamodule import ESM2FineTuneDataModule\n",
    "from bionemo.esm2.model.finetune.finetune_regressor import ESM2FineTuneSeqConfig, InMemorySingleValueDataset\n",
    "from bionemo.llm.model.biobert.lightning import biobert_lightning_module\n",
    "\n",
    "__all__: Sequence[str] = (\"train_model\",)\n",
    "\n",
    "def train_model(\n",
    "    experiment_name: str,\n",
    "    experiment_dir: Path,\n",
    "    config: ESM2GenericConfig,\n",
    "    data_module: pl.LightningDataModule,\n",
    "    n_steps_train: int,\n",
    "    metric_tracker: Callback | None = None,\n",
    "    tokenizer: BioNeMoESMTokenizer = get_tokenizer(),\n",
    "    peft: PEFT | None = None,\n",
    "    _use_rich_model_summary: bool = True,\n",
    ") -> Tuple[Path, Callback | None, nl.Trainer]:\n",
    "    \"\"\"Trains a BioNeMo ESM2 model using PyTorch Lightning.\n",
    "\n",
    "    Parameters:\n",
    "        experiment_name: The name of the experiment.\n",
    "        experiment_dir: The directory where the experiment will be saved.\n",
    "        config: The configuration for the ESM2 model.\n",
    "        data_module: The data module for training and validation.\n",
    "        n_steps_train: The number of training steps.\n",
    "        metric_tracker: Optional callback to track metrics\n",
    "        tokenizer: The tokenizer to use. Defaults to `get_tokenizer()`.\n",
    "        peft: The PEFT (Parameter-Efficient Fine-Tuning) module. Defaults to None.\n",
    "        _use_rich_model_summary: Whether to use the RichModelSummary callback, omitted in our test suite until\n",
    "            https://nvbugspro.nvidia.com/bug/4959776 is resolved. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the path to the saved checkpoint, a MetricTracker\n",
    "        object, and the PyTorch Lightning Trainer object.\n",
    "    \"\"\"\n",
    "    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n",
    "        save_last=True,\n",
    "        save_on_train_epoch_end=True,\n",
    "        monitor=\"reduced_train_loss\",  # TODO find out how to get val_loss logged and use \"val_loss\",\n",
    "        every_n_train_steps=n_steps_train // 2,\n",
    "        always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n",
    "    )\n",
    "\n",
    "    # Setup the logger and train the model\n",
    "    nemo_logger = NeMoLogger(\n",
    "        log_dir=str(experiment_dir),\n",
    "        name=experiment_name,\n",
    "        tensorboard=TensorBoardLogger(save_dir=experiment_dir, name=experiment_name),\n",
    "        ckpt=checkpoint_callback,\n",
    "    )\n",
    "        # Needed so that the trainer can find an output directory for the profiler\n",
    "    # ckpt_path needs to be a string for SerDe\n",
    "    optimizer = MegatronOptimizerModule(\n",
    "        config=OptimizerConfig(\n",
    "            lr=5e-4,\n",
    "            optimizer=\"adam\",\n",
    "            use_distributed_optimizer=True,\n",
    "            fp16=config.fp16,\n",
    "            bf16=config.bf16,\n",
    "        )\n",
    "    )\n",
    "    module = biobert_lightning_module(config=config, tokenizer=tokenizer, optimizer=optimizer, model_transform=peft)\n",
    "\n",
    "    strategy = nl.MegatronStrategy(\n",
    "        tensor_model_parallel_size=1,\n",
    "        pipeline_model_parallel_size=1,\n",
    "        ddp=\"megatron\",\n",
    "        find_unused_parameters=True,\n",
    "        enable_nemo_ckpt_io=True,\n",
    "    )\n",
    "    strategy = 'ddp_notebook'\n",
    "\n",
    "    if _use_rich_model_summary:\n",
    "        # RichModelSummary is not used in the test suite until https://nvbugspro.nvidia.com/bug/4959776 is resolved due\n",
    "        # to errors with serialization / deserialization.\n",
    "        callbacks: list[Callback] = [RichModelSummary(max_depth=4)]\n",
    "    else:\n",
    "        callbacks = []\n",
    "\n",
    "    if metric_tracker is not None:\n",
    "        callbacks.append(metric_tracker)\n",
    "    if peft is not None:\n",
    "        callbacks.append(\n",
    "            ModelTransform()\n",
    "        )  # Callback needed for PEFT fine-tuning using NeMo2, i.e. biobert_lightning_module(model_transform=peft).\n",
    "    trainer = nl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        strategy=strategy,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=n_steps_train // 2,\n",
    "        max_steps=n_steps_train,\n",
    "        num_nodes=1,\n",
    "        log_every_n_steps=n_steps_train // 2,\n",
    "        callbacks=callbacks,\n",
    "        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n",
    "    )\n",
    "    nllm.train(\n",
    "        model=module,\n",
    "        data=data_module,\n",
    "        trainer=trainer,\n",
    "        log=nemo_logger,\n",
    "        resume=resume.AutoResume(\n",
    "            resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n",
    "            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n",
    "        ),\n",
    "    )\n",
    "    ckpt_path = Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n",
    "    return ckpt_path, metric_tracker, trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "144c40fe-c963-4e81-80b0-4e5e330b92d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp_spawn')` is not compatible with an interactive environment. Run your code as a script, or choose a notebook-compatible strategy: `Trainer(strategy='ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m pretrain_ckpt_path \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesm2/650m:2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m config \u001b[38;5;241m=\u001b[39m ESM2FineTuneSeqConfig(initial_ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(pretrain_ckpt_path))\n\u001b[0;32m---> 24\u001b[0m checkpoint, metrics, trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_results_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# new checkpoint will land in a subdir of this\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# same config as before since we are just continuing training\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment completed with checkpoint stored at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 106\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(experiment_name, experiment_dir, config, data_module, n_steps_train, metric_tracker, tokenizer, peft, _use_rich_model_summary)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    104\u001b[0m         ModelTransform()\n\u001b[1;32m    105\u001b[0m     )  \u001b[38;5;66;03m# Callback needed for PEFT fine-tuning using NeMo2, i.e. biobert_lightning_module(model_transform=peft).\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mnl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_val_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_check_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_every_n_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMegatronMixedPrecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbf16-mixed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m nllm\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    119\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodule,\n\u001b[1;32m    120\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata_module,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     ),\n\u001b[1;32m    127\u001b[0m )\n\u001b[1;32m    128\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m Path(checkpoint_callback\u001b[38;5;241m.\u001b[39mlast_model_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/nemo/lightning/io/mixin.py:579\u001b[0m, in \u001b[0;36m_io_wrap_init.<locals>.wrapped_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__io__ \u001b[38;5;241m=\u001b[39m _io_init(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg_kwargs)\n\u001b[0;32m--> 579\u001b[0m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:395\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:162\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_init_precision()\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# 6. Instantiate Strategy - Part 2\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:580\u001b[0m, in \u001b[0;36m_AcceleratorConnector._lazy_init_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_configure_launcher()\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mis_interactive_compatible:\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer(strategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy_flag\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m)` is not compatible with an interactive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m environment. Run your code as a script, or choose a notebook-compatible strategy:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `Trainer(strategy=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mddp_notebook\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m In case you are spawning processes yourself, make sure to include the Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m creation inside the worker function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m     )\n\u001b[1;32m    588\u001b[0m \u001b[38;5;66;03m# TODO: should be moved to _check_strategy_and_fallback().\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Current test check precision first, so keep this check here to meet error order\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, XLAAccelerator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy, (SingleDeviceXLAStrategy, XLAStrategy)\n\u001b[1;32m    592\u001b[0m ):\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp_spawn')` is not compatible with an interactive environment. Run your code as a script, or choose a notebook-compatible strategy: `Trainer(strategy='ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "# set the results directory\n",
    "experiment_results_dir = tempfile.TemporaryDirectory().name\n",
    "\n",
    "# create a List[Tuple] with (sequence, target) values\n",
    "artificial_sequence_data = [\n",
    "    \"TLILGWSDKLGSLLNQLAIANESLGGGTIAVMAERDKEDMELDIGKMEFDFKGTSVI\",\n",
    "    \"SGSKASSDSQDANQCCTSCEDNAPATSYCVECSEPLCETCVEAHQRVKYTKDHTVRSTGPAKT\",\n",
    "]\n",
    "data = [(seq, len(seq) / 100.0) for seq in artificial_sequence_data]\n",
    "\n",
    "# we are training and validating on the same dataset for simplicity\n",
    "dataset = InMemorySingleValueDataset(data)\n",
    "data_module = ESM2FineTuneDataModule(train_dataset=train_dataset, valid_dataset=test_dataset)\n",
    "\n",
    "experiment_name = \"finetune_regressor\"\n",
    "n_steps_train = 50\n",
    "seed = 42\n",
    "\n",
    "# To download a 650M pre-trained ESM2 model\n",
    "pretrain_ckpt_path = load(\"esm2/650m:2.0\")\n",
    "\n",
    "config = ESM2FineTuneSeqConfig(initial_ckpt_path=str(pretrain_ckpt_path))\n",
    "\n",
    "checkpoint, metrics, trainer = train_model(\n",
    "    experiment_name=experiment_name,\n",
    "    experiment_dir=Path(experiment_results_dir),  # new checkpoint will land in a subdir of this\n",
    "    config=config,  # same config as before since we are just continuing training\n",
    "    data_module=data_module,\n",
    "    n_steps_train=n_steps_train,\n",
    ")\n",
    "print(f\"Experiment completed with checkpoint stored at {checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee349a80-56ad-49bc-870e-2af19396d1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module nemo.lightning.pytorch.trainer:\n",
      "\n",
      "class Trainer(lightning.pytorch.trainer.trainer.Trainer, nemo.lightning.io.mixin.IOMixin)\n",
      " |  Trainer(*, accelerator: Union[str, lightning.pytorch.accelerators.accelerator.Accelerator] = 'auto', strategy: Union[str, lightning.pytorch.strategies.strategy.Strategy] = 'auto', devices: Union[List[int], str, int] = 'auto', num_nodes: int = 1, precision: Union[Literal[64, 32, 16], Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16'], NoneType] = None, logger: Union[lightning.pytorch.loggers.logger.Logger, Iterable[lightning.pytorch.loggers.logger.Logger], bool, NoneType] = None, callbacks: Union[List[lightning.pytorch.callbacks.callback.Callback], lightning.pytorch.callbacks.callback.Callback, NoneType] = None, fast_dev_run: Union[int, bool] = False, max_epochs: Optional[int] = None, min_epochs: Optional[int] = None, max_steps: int = -1, min_steps: Optional[int] = None, max_time: Union[str, datetime.timedelta, Dict[str, int], NoneType] = None, limit_train_batches: Union[int, float, NoneType] = None, limit_val_batches: Union[int, float, NoneType] = None, limit_test_batches: Union[int, float, NoneType] = None, limit_predict_batches: Union[int, float, NoneType] = None, overfit_batches: Union[int, float] = 0.0, val_check_interval: Union[int, float, NoneType] = None, check_val_every_n_epoch: Optional[int] = 1, num_sanity_val_steps: Optional[int] = None, log_every_n_steps: Optional[int] = None, enable_checkpointing: Optional[bool] = None, enable_progress_bar: Optional[bool] = None, enable_model_summary: Optional[bool] = None, accumulate_grad_batches: int = 1, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None, deterministic: Union[bool, Literal['warn'], NoneType] = None, benchmark: Optional[bool] = None, inference_mode: bool = True, use_distributed_sampler: bool = True, profiler: Union[lightning.pytorch.profilers.profiler.Profiler, str, NoneType] = None, detect_anomaly: bool = False, barebones: bool = False, plugins: Union[lightning.pytorch.plugins.precision.precision.Precision, lightning.fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning.fabric.plugins.io.checkpoint_io.CheckpointIO, lightning.pytorch.plugins.layer_sync.LayerSync, List[Union[lightning.pytorch.plugins.precision.precision.Precision, lightning.fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning.fabric.plugins.io.checkpoint_io.CheckpointIO, lightning.pytorch.plugins.layer_sync.LayerSync]], NoneType] = None, sync_batchnorm: bool = False, reload_dataloaders_every_n_epochs: int = 0, default_root_dir: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Trainer\n",
      " |      lightning.pytorch.trainer.trainer.Trainer\n",
      " |      nemo.lightning.io.mixin.IOMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *, accelerator: Union[str, lightning.pytorch.accelerators.accelerator.Accelerator] = 'auto', strategy: Union[str, lightning.pytorch.strategies.strategy.Strategy] = 'auto', devices: Union[List[int], str, int] = 'auto', num_nodes: int = 1, precision: Union[Literal[64, 32, 16], Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true'], Literal['64', '32', '16', 'bf16'], NoneType] = None, logger: Union[lightning.pytorch.loggers.logger.Logger, Iterable[lightning.pytorch.loggers.logger.Logger], bool, NoneType] = None, callbacks: Union[List[lightning.pytorch.callbacks.callback.Callback], lightning.pytorch.callbacks.callback.Callback, NoneType] = None, fast_dev_run: Union[int, bool] = False, max_epochs: Optional[int] = None, min_epochs: Optional[int] = None, max_steps: int = -1, min_steps: Optional[int] = None, max_time: Union[str, datetime.timedelta, Dict[str, int], NoneType] = None, limit_train_batches: Union[int, float, NoneType] = None, limit_val_batches: Union[int, float, NoneType] = None, limit_test_batches: Union[int, float, NoneType] = None, limit_predict_batches: Union[int, float, NoneType] = None, overfit_batches: Union[int, float] = 0.0, val_check_interval: Union[int, float, NoneType] = None, check_val_every_n_epoch: Optional[int] = 1, num_sanity_val_steps: Optional[int] = None, log_every_n_steps: Optional[int] = None, enable_checkpointing: Optional[bool] = None, enable_progress_bar: Optional[bool] = None, enable_model_summary: Optional[bool] = None, accumulate_grad_batches: int = 1, gradient_clip_val: Union[int, float, NoneType] = None, gradient_clip_algorithm: Optional[str] = None, deterministic: Union[bool, Literal['warn'], NoneType] = None, benchmark: Optional[bool] = None, inference_mode: bool = True, use_distributed_sampler: bool = True, profiler: Union[lightning.pytorch.profilers.profiler.Profiler, str, NoneType] = None, detect_anomaly: bool = False, barebones: bool = False, plugins: Union[lightning.pytorch.plugins.precision.precision.Precision, lightning.fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning.fabric.plugins.io.checkpoint_io.CheckpointIO, lightning.pytorch.plugins.layer_sync.LayerSync, List[Union[lightning.pytorch.plugins.precision.precision.Precision, lightning.fabric.plugins.environments.cluster_environment.ClusterEnvironment, lightning.fabric.plugins.io.checkpoint_io.CheckpointIO, lightning.pytorch.plugins.layer_sync.LayerSync]], NoneType] = None, sync_batchnorm: bool = False, reload_dataloaders_every_n_epochs: int = 0, default_root_dir: Union[str, pathlib.Path, NoneType] = None) -> None from lightning.pytorch.trainer.trainer.Trainer\n",
      " |      Customize every aspect of training via flags.\n",
      " |\n",
      " |      Args:\n",
      " |          accelerator: Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"hpu\", \"mps\", \"auto\")\n",
      " |              as well as custom accelerator instances.\n",
      " |\n",
      " |          strategy: Supports different training strategies with aliases as well custom strategies.\n",
      " |              Default: ``\"auto\"``.\n",
      " |\n",
      " |          devices: The devices to use. Can be set to a positive number (int or str), a sequence of device indices\n",
      " |              (list or str), the value ``-1`` to indicate all available devices should be used, or ``\"auto\"`` for\n",
      " |              automatic selection based on the chosen accelerator. Default: ``\"auto\"``.\n",
      " |\n",
      " |          num_nodes: Number of GPU nodes for distributed training.\n",
      " |              Default: ``1``.\n",
      " |\n",
      " |          precision: Double precision (64, '64' or '64-true'), full precision (32, '32' or '32-true'),\n",
      " |              16bit mixed precision (16, '16', '16-mixed') or bfloat16 mixed precision ('bf16', 'bf16-mixed').\n",
      " |              Can be used on CPU, GPU, TPUs, or HPUs.\n",
      " |              Default: ``'32-true'``.\n",
      " |\n",
      " |          logger: Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n",
      " |              the default ``TensorBoardLogger`` if it is installed, otherwise ``CSVLogger``.\n",
      " |              ``False`` will disable logging. If multiple loggers are provided, local files\n",
      " |              (checkpoints, profiler traces, etc.) are saved in the ``log_dir`` of the first logger.\n",
      " |              Default: ``True``.\n",
      " |\n",
      " |          callbacks: Add a callback or list of callbacks.\n",
      " |              Default: ``None``.\n",
      " |\n",
      " |          fast_dev_run: Runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      " |              of train, val and test to find any bugs (ie: a sort of unit test).\n",
      " |              Default: ``False``.\n",
      " |\n",
      " |          max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      " |              If both max_epochs and max_steps are not specified, defaults to ``max_epochs = 1000``.\n",
      " |              To enable infinite training, set ``max_epochs = -1``.\n",
      " |\n",
      " |          min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      " |\n",
      " |          max_steps: Stop training after this number of steps. Disabled by default (-1). If ``max_steps = -1``\n",
      " |              and ``max_epochs = None``, will default to ``max_epochs = 1000``. To enable infinite training, set\n",
      " |              ``max_epochs`` to ``-1``.\n",
      " |\n",
      " |          min_steps: Force training for at least these number of steps. Disabled by default (``None``).\n",
      " |\n",
      " |          max_time: Stop training after this amount of time has passed. Disabled by default (``None``).\n",
      " |              The time duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n",
      " |              :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n",
      " |              :class:`datetime.timedelta`.\n",
      " |\n",
      " |          limit_train_batches: How much of training dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |\n",
      " |          limit_val_batches: How much of validation dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |\n",
      " |          limit_test_batches: How much of test dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |\n",
      " |          limit_predict_batches: How much of prediction dataset to check (float = fraction, int = num_batches).\n",
      " |              Default: ``1.0``.\n",
      " |\n",
      " |          overfit_batches: Overfit a fraction of training/validation data (float) or a set number of batches (int).\n",
      " |              Default: ``0.0``.\n",
      " |\n",
      " |          val_check_interval: How often to check the validation set. Pass a ``float`` in the range [0.0, 1.0] to check\n",
      " |              after a fraction of the training epoch. Pass an ``int`` to check after a fixed number of training\n",
      " |              batches. An ``int`` value can only be higher than the number of training batches when\n",
      " |              ``check_val_every_n_epoch=None``, which validates after every ``N`` training batches\n",
      " |              across epochs or during iteration-based training.\n",
      " |              Default: ``1.0``.\n",
      " |\n",
      " |          check_val_every_n_epoch: Perform a validation loop after every `N` training epochs. If ``None``,\n",
      " |              validation will be done solely based on the number of training batches, requiring ``val_check_interval``\n",
      " |              to be an integer value.\n",
      " |              Default: ``1``.\n",
      " |\n",
      " |          num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      " |              Set it to `-1` to run all batches in all validation dataloaders.\n",
      " |              Default: ``2``.\n",
      " |\n",
      " |          log_every_n_steps: How often to log within steps.\n",
      " |              Default: ``50``.\n",
      " |\n",
      " |          enable_checkpointing: If ``True``, enable checkpointing.\n",
      " |              It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.callbacks`.\n",
      " |              Default: ``True``.\n",
      " |\n",
      " |          enable_progress_bar: Whether to enable to progress bar by default.\n",
      " |              Default: ``True``.\n",
      " |\n",
      " |          enable_model_summary: Whether to enable model summarization by default.\n",
      " |              Default: ``True``.\n",
      " |\n",
      " |          accumulate_grad_batches: Accumulates gradients over k batches before stepping the optimizer.\n",
      " |              Default: 1.\n",
      " |\n",
      " |          gradient_clip_val: The value at which to clip gradients. Passing ``gradient_clip_val=None`` disables\n",
      " |              gradient clipping. If using Automatic Mixed Precision (AMP), the gradients will be unscaled before.\n",
      " |              Default: ``None``.\n",
      " |\n",
      " |          gradient_clip_algorithm: The gradient clipping algorithm to use. Pass ``gradient_clip_algorithm=\"value\"``\n",
      " |              to clip by value, and ``gradient_clip_algorithm=\"norm\"`` to clip by norm. By default it will\n",
      " |              be set to ``\"norm\"``.\n",
      " |\n",
      " |          deterministic: If ``True``, sets whether PyTorch operations must use deterministic algorithms.\n",
      " |              Set to ``\"warn\"`` to use deterministic algorithms whenever possible, throwing warnings on operations\n",
      " |              that don't support deterministic mode. If not set, defaults to ``False``. Default: ``None``.\n",
      " |\n",
      " |          benchmark: The value (``True`` or ``False``) to set ``torch.backends.cudnn.benchmark`` to.\n",
      " |              The value for ``torch.backends.cudnn.benchmark`` set in the current session will be used\n",
      " |              (``False`` if not manually set). If :paramref:`~lightning.pytorch.trainer.trainer.Trainer.deterministic`\n",
      " |              is set to ``True``, this will default to ``False``. Override to manually set a different value.\n",
      " |              Default: ``None``.\n",
      " |\n",
      " |          inference_mode: Whether to use :func:`torch.inference_mode` or :func:`torch.no_grad` during\n",
      " |              evaluation (``validate``/``test``/``predict``).\n",
      " |\n",
      " |          use_distributed_sampler: Whether to wrap the DataLoader's sampler with\n",
      " |              :class:`torch.utils.data.DistributedSampler`. If not specified this is toggled automatically for\n",
      " |              strategies that require it. By default, it will add ``shuffle=True`` for the train sampler and\n",
      " |              ``shuffle=False`` for validation/test/predict samplers. If you want to disable this logic, you can pass\n",
      " |              ``False`` and add your own distributed sampler in the dataloader hooks. If ``True`` and a distributed\n",
      " |              sampler was already added, Lightning will not replace the existing one. For iterable-style datasets,\n",
      " |              we don't do this automatically.\n",
      " |\n",
      " |          profiler: To profile individual steps during training and assist in identifying bottlenecks.\n",
      " |              Default: ``None``.\n",
      " |\n",
      " |          detect_anomaly: Enable anomaly detection for the autograd engine.\n",
      " |              Default: ``False``.\n",
      " |\n",
      " |          barebones: Whether to run in \"barebones mode\", where all features that may impact raw speed are\n",
      " |              disabled. This is meant for analyzing the Trainer overhead and is discouraged during regular training\n",
      " |              runs. The following features are deactivated:\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_checkpointing`,\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.logger`,\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_progress_bar`,\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.log_every_n_steps`,\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.enable_model_summary`,\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.num_sanity_val_steps`,\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.fast_dev_run`,\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.detect_anomaly`,\n",
      " |              :paramref:`~lightning.pytorch.trainer.trainer.Trainer.profiler`,\n",
      " |              :meth:`~lightning.pytorch.core.LightningModule.log`,\n",
      " |              :meth:`~lightning.pytorch.core.LightningModule.log_dict`.\n",
      " |          plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      " |              Default: ``None``.\n",
      " |\n",
      " |          sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      " |              Default: ``False``.\n",
      " |\n",
      " |          reload_dataloaders_every_n_epochs: Set to a positive integer to reload dataloaders every n epochs.\n",
      " |              Default: ``0``.\n",
      " |\n",
      " |          default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      " |              Default: ``os.getcwd()``.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``gradient_clip_val`` is not an int or float.\n",
      " |\n",
      " |          MisconfigurationException:\n",
      " |              If ``gradient_clip_algorithm`` is invalid.\n",
      " |\n",
      " |  add_io(self, obj)\n",
      " |      Recurse to the leaves of a container and add io functionality to non-serializable leaves\n",
      " |\n",
      " |  io_init(self, **kwargs) -> fiddle._src.config.Config[typing.Self]\n",
      " |      Initializes the configuration object (`__io__`) with the captured arguments.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs: A dictionary of arguments that were captured during object initialization.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |          fdl.Config[Self]: The initialized configuration object.\n",
      " |\n",
      " |  to_fabric(self, callbacks=None, loggers=None) -> nemo.lightning.fabric.fabric.Fabric\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  __wrapped_init__ = True\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from lightning.pytorch.trainer.trainer.Trainer:\n",
      " |\n",
      " |  fit(self, model: 'pl.LightningModule', train_dataloaders: Union[Any, lightning.pytorch.core.datamodule.LightningDataModule, NoneType] = None, val_dataloaders: Optional[Any] = None, datamodule: Optional[lightning.pytorch.core.datamodule.LightningDataModule] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None) -> None\n",
      " |      Runs the full optimization routine.\n",
      " |\n",
      " |      Args:\n",
      " |          model: Model to fit.\n",
      " |\n",
      " |          train_dataloaders: An iterable or collection of iterables specifying training samples.\n",
      " |              Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\n",
      " |\n",
      " |          val_dataloaders: An iterable or collection of iterables specifying validation samples.\n",
      " |\n",
      " |          datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~lightning.pytorch.core.hooks.DataHooks.train_dataloader` hook.\n",
      " |\n",
      " |          ckpt_path: Path/URL of the checkpoint from which training is resumed. Could also be one of two special\n",
      " |              keywords ``\"last\"`` and ``\"hpc\"``. If there is no checkpoint file at the path, an exception is raised.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``model`` is not :class:`~lightning.pytorch.core.LightningModule` for torch version less than\n",
      " |              2.0.0 and if ``model`` is not :class:`~lightning.pytorch.core.LightningModule` or\n",
      " |              :class:`torch._dynamo.OptimizedModule` for torch versions greater than or equal to 2.0.0 .\n",
      " |\n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |\n",
      " |  init_module(self, empty_init: Optional[bool] = None) -> Generator\n",
      " |      Tensors that you instantiate under this context manager will be created on the device right away and have\n",
      " |      the right data type depending on the precision setting in the Trainer.\n",
      " |\n",
      " |      The parameters and tensors get created on the device and with the right data type right away without wasting\n",
      " |      memory being allocated unnecessarily.\n",
      " |\n",
      " |      Args:\n",
      " |          empty_init: Whether to initialize the model with empty weights (uninitialized memory).\n",
      " |              If ``None``, the strategy will decide. Some strategies may not support all options.\n",
      " |              Set this to ``True`` if you are loading a checkpoint into a large model.\n",
      " |\n",
      " |  predict(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, lightning.pytorch.core.datamodule.LightningDataModule, NoneType] = None, datamodule: Optional[lightning.pytorch.core.datamodule.LightningDataModule] = None, return_predictions: Optional[bool] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None) -> Union[List[Any], List[List[Any]], NoneType]\n",
      " |      Run inference on your data. This will call the model forward function to compute predictions. Useful to\n",
      " |      perform distributed and batched predictions. Logging is disabled in the predict hooks.\n",
      " |\n",
      " |      Args:\n",
      " |          model: The model to predict with.\n",
      " |\n",
      " |          dataloaders: An iterable or collection of iterables specifying predict samples.\n",
      " |              Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\n",
      " |\n",
      " |          datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~lightning.pytorch.core.hooks.DataHooks.predict_dataloader` hook.\n",
      " |\n",
      " |          return_predictions: Whether to return predictions.\n",
      " |              ``True`` by default except when an accelerator that spawns processes is used (not supported).\n",
      " |\n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to predict.\n",
      " |              If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |\n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |\n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |\n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |\n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |\n",
      " |      See :ref:`Lightning inference section<deploy/production_basic:Predict step with your LightningModule>` for more.\n",
      " |\n",
      " |  print(self, *args: Any, **kwargs: Any) -> None\n",
      " |      Print something only on the first process. If running on multiple machines, it will print from the first\n",
      " |      process in each machine.\n",
      " |\n",
      " |      Arguments passed to this method are forwarded to the Python built-in :func:`print` function.\n",
      " |\n",
      " |  save_checkpoint(self, filepath: Union[str, pathlib.Path], weights_only: bool = False, storage_options: Optional[Any] = None) -> None\n",
      " |      Runs routine to create a checkpoint.\n",
      " |\n",
      " |      This method needs to be called on all processes in case the selected strategy is handling distributed\n",
      " |      checkpointing.\n",
      " |\n",
      " |      Args:\n",
      " |          filepath: Path where checkpoint is saved.\n",
      " |          weights_only: If ``True``, will only save the model weights.\n",
      " |          storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError:\n",
      " |              If the model is not attached to the Trainer before calling this method.\n",
      " |\n",
      " |  test(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, lightning.pytorch.core.datamodule.LightningDataModule, NoneType] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None, verbose: bool = True, datamodule: Optional[lightning.pytorch.core.datamodule.LightningDataModule] = None) -> List[Mapping[str, float]]\n",
      " |      Perform one evaluation epoch over the test set. It's separated from fit to make sure you never run on your\n",
      " |      test set until you want to.\n",
      " |\n",
      " |      Args:\n",
      " |          model: The model to test.\n",
      " |\n",
      " |          dataloaders: An iterable or collection of iterables specifying test samples.\n",
      " |              Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\n",
      " |\n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to test.\n",
      " |              If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |\n",
      " |          verbose: If True, prints the test results.\n",
      " |\n",
      " |          datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~lightning.pytorch.core.hooks.DataHooks.test_dataloader` hook.\n",
      " |\n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |\n",
      " |      Returns:\n",
      " |          List of dictionaries with metrics logged during the test phase, e.g., in model- or callback hooks\n",
      " |          like :meth:`~lightning.pytorch.LightningModule.test_step` etc.\n",
      " |          The length of the list corresponds to the number of test dataloaders used.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |\n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |\n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |\n",
      " |  validate(self, model: Optional[ForwardRef('pl.LightningModule')] = None, dataloaders: Union[Any, lightning.pytorch.core.datamodule.LightningDataModule, NoneType] = None, ckpt_path: Union[str, pathlib.Path, NoneType] = None, verbose: bool = True, datamodule: Optional[lightning.pytorch.core.datamodule.LightningDataModule] = None) -> List[Mapping[str, float]]\n",
      " |      Perform one evaluation epoch over the validation set.\n",
      " |\n",
      " |      Args:\n",
      " |          model: The model to validate.\n",
      " |\n",
      " |          dataloaders: An iterable or collection of iterables specifying validation samples.\n",
      " |              Alternatively, a :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\n",
      " |\n",
      " |          ckpt_path: Either ``\"best\"``, ``\"last\"``, ``\"hpc\"`` or path to the checkpoint you wish to validate.\n",
      " |              If ``None`` and the model instance was passed, use the current weights.\n",
      " |              Otherwise, the best model checkpoint from the previous ``trainer.fit`` call will be loaded\n",
      " |              if a checkpoint callback is configured.\n",
      " |\n",
      " |          verbose: If True, prints the validation results.\n",
      " |\n",
      " |          datamodule: A :class:`~lightning.pytorch.core.datamodule.LightningDataModule` that defines\n",
      " |              the :class:`~lightning.pytorch.core.hooks.DataHooks.val_dataloader` hook.\n",
      " |\n",
      " |      For more information about multiple dataloaders, see this :ref:`section <multiple-dataloaders>`.\n",
      " |\n",
      " |      Returns:\n",
      " |          List of dictionaries with metrics logged during the validation phase, e.g., in model- or callback hooks\n",
      " |          like :meth:`~lightning.pytorch.LightningModule.validation_step` etc.\n",
      " |          The length of the list corresponds to the number of validation dataloaders used.\n",
      " |\n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If no ``model`` is passed and there was no ``LightningModule`` passed in the previous run.\n",
      " |              If ``model`` passed is not `LightningModule` or `torch._dynamo.OptimizedModule`.\n",
      " |\n",
      " |          MisconfigurationException:\n",
      " |              If both ``dataloaders`` and ``datamodule`` are passed. Pass only one of these.\n",
      " |\n",
      " |          RuntimeError:\n",
      " |              If a compiled ``model`` is passed and the strategy is not supported.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from lightning.pytorch.trainer.trainer.Trainer:\n",
      " |\n",
      " |  accelerator\n",
      " |\n",
      " |  callback_metrics\n",
      " |      The metrics available to callbacks.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              self.log(\"a_val\", 2.0)\n",
      " |\n",
      " |\n",
      " |          callback_metrics = trainer.callback_metrics\n",
      " |          assert callback_metrics[\"a_val\"] == 2.0\n",
      " |\n",
      " |  checkpoint_callback\n",
      " |      The first :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` callback in the\n",
      " |      Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |\n",
      " |  checkpoint_callbacks\n",
      " |      A list of all instances of :class:`~lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint` found in\n",
      " |      the Trainer.callbacks list.\n",
      " |\n",
      " |  current_epoch\n",
      " |      The current epoch, updated after the epoch end hooks are run.\n",
      " |\n",
      " |  default_root_dir\n",
      " |      The default location to save artifacts of loggers, checkpoints etc.\n",
      " |\n",
      " |      It is used as a fallback if logger or checkpoint callback do not define specific save paths.\n",
      " |\n",
      " |  device_ids\n",
      " |      List of device indexes per node.\n",
      " |\n",
      " |  distributed_sampler_kwargs\n",
      " |\n",
      " |  early_stopping_callback\n",
      " |      The first :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` callback in the\n",
      " |      Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |\n",
      " |  early_stopping_callbacks\n",
      " |      A list of all instances of :class:`~lightning.pytorch.callbacks.early_stopping.EarlyStopping` found in the\n",
      " |      Trainer.callbacks list.\n",
      " |\n",
      " |  enable_validation\n",
      " |      Check if we should run validation during training.\n",
      " |\n",
      " |  estimated_stepping_batches\n",
      " |      The estimated number of batches that will ``optimizer.step()`` during training.\n",
      " |\n",
      " |      This accounts for gradient accumulation and the current trainer configuration. This might be used when setting\n",
      " |      up your training dataloader, if it hasn't been set up already.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def configure_optimizers(self):\n",
      " |              optimizer = ...\n",
      " |              stepping_batches = self.trainer.estimated_stepping_batches\n",
      " |              scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\n",
      " |              return [optimizer], [scheduler]\n",
      " |\n",
      " |      Raises:\n",
      " |          MisconfigurationException:\n",
      " |              If estimated stepping batches cannot be computed due to different `accumulate_grad_batches`\n",
      " |              at different epochs.\n",
      " |\n",
      " |  evaluating\n",
      " |\n",
      " |  global_rank\n",
      " |\n",
      " |  global_step\n",
      " |      The number of optimizer steps taken (does not reset each epoch).\n",
      " |\n",
      " |      This includes multiple optimizers (if enabled).\n",
      " |\n",
      " |  interrupted\n",
      " |\n",
      " |  is_global_zero\n",
      " |      Whether this process is the global zero in multi-node training.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def training_step(self, batch, batch_idx):\n",
      " |              if self.trainer.is_global_zero:\n",
      " |                  print(\"in node 0, accelerator 0\")\n",
      " |\n",
      " |  is_last_batch\n",
      " |      Whether trainer is executing the last batch.\n",
      " |\n",
      " |  lightning_module\n",
      " |\n",
      " |  local_rank\n",
      " |\n",
      " |  log_dir\n",
      " |      The directory for the current experiment. Use this to save images to, etc...\n",
      " |\n",
      " |      .. note:: You must call this on all processes. Failing to do so will cause your program to stall forever.\n",
      " |\n",
      " |       .. code-block:: python\n",
      " |\n",
      " |           def training_step(self, batch, batch_idx):\n",
      " |               img = ...\n",
      " |               save_img(img, self.trainer.log_dir)\n",
      " |\n",
      " |  logged_metrics\n",
      " |      The metrics sent to the loggers.\n",
      " |\n",
      " |      This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\n",
      " |      :paramref:`~lightning.pytorch.core.LightningModule.log.logger` argument set.\n",
      " |\n",
      " |  lr_scheduler_configs\n",
      " |\n",
      " |  max_epochs\n",
      " |\n",
      " |  max_steps\n",
      " |\n",
      " |  min_epochs\n",
      " |\n",
      " |  min_steps\n",
      " |\n",
      " |  model\n",
      " |      The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\n",
      " |\n",
      " |      To access the pure LightningModule, use\n",
      " |      :meth:`~lightning.pytorch.trainer.trainer.Trainer.lightning_module` instead.\n",
      " |\n",
      " |  node_rank\n",
      " |\n",
      " |  num_devices\n",
      " |      Number of devices the trainer uses per node.\n",
      " |\n",
      " |  num_nodes\n",
      " |\n",
      " |  num_predict_batches\n",
      " |      The number of prediction batches that will be used during ``trainer.predict()``.\n",
      " |\n",
      " |  num_sanity_val_batches\n",
      " |      The number of validation batches that will be used during the sanity-checking part of ``trainer.fit()``.\n",
      " |\n",
      " |  num_test_batches\n",
      " |      The number of test batches that will be used during ``trainer.test()``.\n",
      " |\n",
      " |  num_training_batches\n",
      " |      The number of training batches that will be used during ``trainer.fit()``.\n",
      " |\n",
      " |  num_val_batches\n",
      " |      The number of validation batches that will be used during ``trainer.fit()`` or ``trainer.validate()``.\n",
      " |\n",
      " |  precision\n",
      " |\n",
      " |  precision_plugin\n",
      " |\n",
      " |  predict_dataloaders\n",
      " |      The prediction dataloader(s) used during ``trainer.predict()``.\n",
      " |\n",
      " |  progress_bar_callback\n",
      " |      An instance of :class:`~lightning.pytorch.callbacks.progress.progress_bar.ProgressBar` found in the\n",
      " |      Trainer.callbacks list, or ``None`` if one doesn't exist.\n",
      " |\n",
      " |  progress_bar_metrics\n",
      " |      The metrics sent to the progress bar.\n",
      " |\n",
      " |      This includes metrics logged via :meth:`~lightning.pytorch.core.LightningModule.log` with the\n",
      " |      :paramref:`~lightning.pytorch.core.LightningModule.log.prog_bar` argument set.\n",
      " |\n",
      " |  received_sigterm\n",
      " |      Whether a ``signal.SIGTERM`` signal was received.\n",
      " |\n",
      " |      For example, this can be checked to exit gracefully.\n",
      " |\n",
      " |  scaler\n",
      " |\n",
      " |  strategy\n",
      " |\n",
      " |  test_dataloaders\n",
      " |      The test dataloader(s) used during ``trainer.test()``.\n",
      " |\n",
      " |  train_dataloader\n",
      " |      The training dataloader(s) used during ``trainer.fit()``.\n",
      " |\n",
      " |  val_dataloaders\n",
      " |      The validation dataloader(s) used during ``trainer.fit()`` or ``trainer.validate()``.\n",
      " |\n",
      " |  world_size\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from lightning.pytorch.trainer.trainer.Trainer:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ckpt_path\n",
      " |      Set to the path/URL of a checkpoint loaded via :meth:`~lightning.pytorch.trainer.trainer.Trainer.fit`,\n",
      " |      :meth:`~lightning.pytorch.trainer.trainer.Trainer.validate`,\n",
      " |      :meth:`~lightning.pytorch.trainer.trainer.Trainer.test`, or\n",
      " |      :meth:`~lightning.pytorch.trainer.trainer.Trainer.predict`.\n",
      " |\n",
      " |      ``None`` otherwise.\n",
      " |\n",
      " |  logger\n",
      " |      The first :class:`~lightning.pytorch.loggers.logger.Logger` being used.\n",
      " |\n",
      " |  loggers\n",
      " |      The list of :class:`~lightning.pytorch.loggers.logger.Logger` used.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          for logger in trainer.loggers:\n",
      " |              logger.log_metrics({\"foo\": 1.0})\n",
      " |\n",
      " |  optimizers\n",
      " |\n",
      " |  predicting\n",
      " |\n",
      " |  sanity_checking\n",
      " |      Whether sanity checking is running.\n",
      " |\n",
      " |      Useful to disable some hooks, logging or callbacks during the sanity checking.\n",
      " |\n",
      " |  testing\n",
      " |\n",
      " |  training\n",
      " |\n",
      " |  validating\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nemo.lightning.io.mixin.IOMixin:\n",
      " |\n",
      " |  io_dump(self, output: pathlib.Path, yaml_attrs: list[str])\n",
      " |      Serializes the configuration object (`__io__`) to a file, allowing the object state to be\n",
      " |      saved and later restored. Also creates an artifacts directory and stores it in a thread-local\n",
      " |      global variable. If the artifacts directory is empty at the end, it is deleted.\n",
      " |\n",
      " |      Args:\n",
      " |          output (Path): The path to the directory where the configuration object and artifacts\n",
      " |                         will be stored.\n",
      " |\n",
      " |  io_transform_args(self, init_fn, *args, **kwargs) -> Dict[str, Any]\n",
      " |      Transforms and captures the arguments passed to the `__init__` method, filtering out\n",
      " |      any arguments that are instances of `IOProtocol` or are dataclass fields with default\n",
      " |      factories.\n",
      " |\n",
      " |      Args:\n",
      " |          init_fn (Callable): The original `__init__` method of the class.\n",
      " |          *args: Variable length argument list for the `__init__` method.\n",
      " |          **kwargs: Arbitrary keyword arguments for the `__init__` method.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |          Dict[str, Any]: A dictionary of the captured and transformed arguments.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from nemo.lightning.io.mixin.IOMixin:\n",
      " |\n",
      " |  __init_subclass__()\n",
      " |      This method is called when a class is subclassed.\n",
      " |\n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |\n",
      " |  io_artifacts() -> List[nemo.lightning.io.artifact.base.Artifact]\n",
      " |      Initialize io artifacts\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from nemo.lightning.io.mixin.IOMixin:\n",
      " |\n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Overrides the default object creation process to wrap the `__init__` method, allowing\n",
      " |      initialization arguments to be captured and stored in the `__io__` attribute.\n",
      " |\n",
      " |      Args:\n",
      " |          *args: Variable length argument list for the `__init__` method.\n",
      " |          **kwargs: Arbitrary keyword arguments for the `__init__` method.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |          The newly created object instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nl.Trainer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
