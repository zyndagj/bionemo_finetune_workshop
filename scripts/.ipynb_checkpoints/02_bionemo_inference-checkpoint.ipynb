{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4373abfb-e7d3-4560-8d28-dd19793bdb58",
   "metadata": {},
   "source": [
    "# BioNeMo Inference\n",
    "\n",
    "## Documentation\n",
    "- https://github.com/NVIDIA/bionemo-framework/blob/v2.3/docs/docs/user-guide/examples/bionemo-esm2/inference.ipynb\n",
    "- https://docs.nvidia.com/bionemo-framework/latest/user-guide/examples/bionemo-esm2/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5549b6-acee-4c42-82b7-0560ff59b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this checkpoint after fine-tuning\n",
    "checkpoint_path = \"/tmp/tmp2ayq5vmh/finetune_regressor/checkpoints/finetune_regressor--reduced_train_loss=0.0249-epoch=0-consumed_samples=1600.0-last\"\n",
    "\n",
    "# This gets created after running 01_hf_fine-tune.ipynb\n",
    "valid_csv = \"/tmp/test_df.csv\"\n",
    "work_dir = \"/tmp/work\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60332749-0aea-4579-8199-d664fc810f66",
   "metadata": {},
   "source": [
    "Similar to fine-tuning, BioNeMo inference mush be run in a separate process, which is easy to do through calling a shell with `!` in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1505b100-0f49-442e-a2f3-78718f2c47f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-18 05:19:03 - faiss.loader - INFO - Loading faiss with AVX2 support.\n",
      "2025-02-18 05:19:03 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n",
      "[NeMo W 2025-02-18 05:19:03 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "2025-02-18 05:19:04 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
      "2025-02-18 05:19:04 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
      "2025-02-18 05:19:04 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] All embedding group ranks: [[0]]\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Rank 0 has embedding rank: 0\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "2025-02-18 05:19:04 - pytorch_lightning.utilities.rank_zero - INFO - ----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "2025-02-18 05:19:04 - /usr/local/lib/python3.12/dist-packages/bionemo/llm/model/config.py - WARNING - Loading /tmp/tmp2ayq5vmh/finetune_regressor/checkpoints/finetune_regressor--reduced_train_loss=0.0249-epoch=0-consumed_samples=1600.0-last\n",
      "[NeMo I 2025-02-18 05:19:04 nemo_logging:393] Padded vocab_size: 128, original vocab_size: 33, dummy tokens: 95.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2025-02-18 05:19:05 nemo_logging:405] Could not copy Trainer's 'max_steps' to LR scheduler's 'max_steps'. If you are not using an LR scheduler, this warning can safely be ignored.\n",
      "2025-02-18 05:19:05 - megatron.core.num_microbatches_calculator - INFO - setting number of microbatches to constant 1\n",
      "[NeMo I 2025-02-18 05:19:05 nemo_logging:393]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 651492481\n",
      "[NeMo I 2025-02-18 05:19:05 nemo_logging:393]  > number of trainable parameters: 328193 (0.05% of total)\n",
      "2025-02-18 05:19:05 - megatron.core.distributed.distributed_data_parallel - INFO - Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)\n",
      "2025-02-18 05:19:05 - megatron.core.distributed.param_and_grad_buffer - INFO - Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "Params for bucket 1 (328193 elements):\n",
      "\tmodule.regression_head.linear_layers.1.bias\n",
      "\tmodule.regression_head.linear_layers.0.weight\n",
      "\tmodule.regression_head.linear_layers.0.bias\n",
      "\tmodule.regression_head.linear_layers.1.weight\n",
      "2025-02-18 05:19:05 - megatron.core.optimizer - INFO - Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0001, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.01, fp16=False, bf16=True, params_dtype=torch.float32, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "2025-02-18 05:19:05 - root - INFO - Instantiating MegatronPretrainingSampler with total_samples: 1296 and consumed_samples: 0\n",
      "2025-02-18 05:19:40 - root - INFO - Inference predictions are stored in /tmp/work/predictions__rank_0.pt\n",
      "dict_keys(['token_logits', 'binary_logits', 'hidden_states', 'input_ids', 'embeddings', 'regression_output'])\n"
     ]
    }
   ],
   "source": [
    "! infer_esm2 --checkpoint-path {checkpoint_path} \\\n",
    "             --data-path {valid_csv} \\\n",
    "             --results-path {work_dir} \\\n",
    "             --micro-batch-size 3 \\\n",
    "             --num-gpus 1 \\\n",
    "             --precision \"bf16-mixed\" \\\n",
    "             --include-hiddens \\\n",
    "             --include-embeddings \\\n",
    "             --include-logits \\\n",
    "             --include-input-ids \\\n",
    "             --config-class ESM2FineTuneSeqConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c227bdb2-17ac-43f5-a9df-f40b46ebdcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_187/1245991107.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  results = torch.load(f\"{work_dir}/predictions__rank_0.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_logits\ttorch.Size([1024, 1296, 128])\n",
      "hidden_states\ttorch.Size([1296, 1024, 1280])\n",
      "input_ids\ttorch.Size([1296, 1024])\n",
      "embeddings\ttorch.Size([1296, 1280])\n",
      "regression_output\ttorch.Size([1296, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "results = torch.load(f\"{work_dir}/predictions__rank_0.pt\")\n",
    "\n",
    "# Print out the contents of inference results\n",
    "for key, val in results.items():\n",
    "    if val is not None:\n",
    "        print(f'{key}\\t{val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee5ac0e-6951-41a3-ba24-10378bd300b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequences</th>\n",
       "      <th>labels</th>\n",
       "      <th>inference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MVDREQLVQKARLAEQAERYDDMAAAMKNVTELNEPLSNEERNLLS...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MVKGEKGPKGKKITLKVARNCIKITFDGKKRLDLSKMGITTFPKCI...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEPLRAPALRRLLPPLLLLLLSLPPRARAKYVRGNLSSKEDWVFLT...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MGNHAGKRELNAEKASTNSETNRGESEKKRNLGELSRTTSEDNEVF...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MPKVVSRSVVCSDTRDREEYDDGEKPLHVYYCLCGQMVLVLDCQLE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>MALTPTNLNNKMSLQMKMDCQEQQLTKKNNGFFQKLNVTEGAMQDL...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>MASPSNDSTAPVSEFLLICFPNFQSWQHWLSLPLSLLFLLAMGANT...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>MAWTPLWLTLLTLCIGSVVSSELTQDPAVSVALGQTVRITCQGDSL...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>MDTLCSTLLLLTIPSWVLSQITLKESGPTLVKPTQTLTLTCTFSGF...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>MALVFRTVAQLAGVSLSLLGWVLSCLTNYLPHWKNLNLDLNEMENW...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1296 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sequences  labels  inference\n",
       "0     MVDREQLVQKARLAEQAERYDDMAAAMKNVTELNEPLSNEERNLLS...       0          0\n",
       "1     MVKGEKGPKGKKITLKVARNCIKITFDGKKRLDLSKMGITTFPKCI...       0          0\n",
       "2     MEPLRAPALRRLLPPLLLLLLSLPPRARAKYVRGNLSSKEDWVFLT...       1          1\n",
       "3     MGNHAGKRELNAEKASTNSETNRGESEKKRNLGELSRTTSEDNEVF...       0          0\n",
       "4     MPKVVSRSVVCSDTRDREEYDDGEKPLHVYYCLCGQMVLVLDCQLE...       0          0\n",
       "...                                                 ...     ...        ...\n",
       "1291  MALTPTNLNNKMSLQMKMDCQEQQLTKKNNGFFQKLNVTEGAMQDL...       1          0\n",
       "1292  MASPSNDSTAPVSEFLLICFPNFQSWQHWLSLPLSLLFLLAMGANT...       1          1\n",
       "1293  MAWTPLWLTLLTLCIGSVVSSELTQDPAVSVALGQTVRITCQGDSL...       1          1\n",
       "1294  MDTLCSTLLLLTIPSWVLSQITLKESGPTLVKPTQTLTLTCTFSGF...       1          1\n",
       "1295  MALVFRTVAQLAGVSLSLLGWVLSCLTNYLPHWKNLNLDLNEMENW...       1          1\n",
       "\n",
       "[1296 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# Print out the original sequence, true label, and inferred label\n",
    "test_df = pandas.read_csv(\"/tmp/test_df.csv\")[['sequences','labels']]\n",
    "test_df['inference'] = results['regression_output'].round().int().numpy()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe64909-74a4-4519-a4db-e764043843a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398148148148148"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_df['labels'], test_df['inference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846e98c-ff27-4242-a2f5-61dc158e431a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
