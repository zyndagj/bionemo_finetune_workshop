{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5549b6-acee-4c42-82b7-0560ff59b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "import requests\n",
    "\n",
    "query_url =\"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Ccc_subcellular_location&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B80%20TO%20500%5D%29%29\"\n",
    "query_url =\"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Ccc_subcellular_location&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B80%20TO%20500%5D%29%29\"\n",
    "\n",
    "uniprot_request = requests.get(query_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee5ac0e-6951-41a3-ba24-10378bd300b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0AVI4</td>\n",
       "      <td>MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Endoplasmic reticulum me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0JLT2</td>\n",
       "      <td>MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Nucleus {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6</td>\n",
       "      <td>GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0PJY2</td>\n",
       "      <td>MDSSCHNATTKMLATAPARGNMMSTSKPLAFSIERIMARTPEPKAL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Nucleus {ECO:0000269|Pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11972</th>\n",
       "      <td>Q9H8W2</td>\n",
       "      <td>MRPGSSPRAPECGAPALPRPQLDRLPARPAPSRGRGAPSLRWPAKE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11973</th>\n",
       "      <td>Q9HAA7</td>\n",
       "      <td>MLFGIRILVNTPSPLVTGLHHYNPSIHRDQGECANQWRKGPGSAHL...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11974</th>\n",
       "      <td>Q9NZ38</td>\n",
       "      <td>MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>Q9UFV3</td>\n",
       "      <td>MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11976</th>\n",
       "      <td>Q9Y6C7</td>\n",
       "      <td>MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11977 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "1          A0AVI4  MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...   \n",
       "2          A0JLT2  MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...   \n",
       "3          A0M8Q6  GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...   \n",
       "4          A0PJY2  MDSSCHNATTKMLATAPARGNMMSTSKPLAFSIERIMARTPEPKAL...   \n",
       "...           ...                                                ...   \n",
       "11972      Q9H8W2  MRPGSSPRAPECGAPALPRPQLDRLPARPAPSRGRGAPSLRWPAKE...   \n",
       "11973      Q9HAA7  MLFGIRILVNTPSPLVTGLHHYNPSIHRDQGECANQWRKGPGSAHL...   \n",
       "11974      Q9NZ38  MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...   \n",
       "11975      Q9UFV3  MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...   \n",
       "11976      Q9Y6C7  MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "0      SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...  \n",
       "1      SUBCELLULAR LOCATION: Endoplasmic reticulum me...  \n",
       "2           SUBCELLULAR LOCATION: Nucleus {ECO:0000305}.  \n",
       "3      SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...  \n",
       "4      SUBCELLULAR LOCATION: Nucleus {ECO:0000269|Pub...  \n",
       "...                                                  ...  \n",
       "11972                                                NaN  \n",
       "11973                                                NaN  \n",
       "11974                                                NaN  \n",
       "11975                                                NaN  \n",
       "11976                                                NaN  \n",
       "\n",
       "[11977 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import pandas\n",
    "\n",
    "bio = BytesIO(uniprot_request.content)\n",
    "\n",
    "df = pandas.read_csv(bio, compression='gzip', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4ae7c9-4161-40c9-850d-6d5036f022fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A1E959</td>\n",
       "      <td>MKIIILLGFLGATLSAPLIPQRLMSASNSNELLLNLNNGQLLPLQL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000250|Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A1XBS5</td>\n",
       "      <td>MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A2RU49</td>\n",
       "      <td>MSSGNYQQSEALSKPTFSEEQASALVESVFGLKVSKVRPLPSYDDQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A2RUH7</td>\n",
       "      <td>MEAATAPEVAAGSKLKVKEASPADAEPPQASPGQGAGSPTPQLLPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, myofibril, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A4D126</td>\n",
       "      <td>MEAGPPGSARPAEPGPCLSGQRGADHTASASLQSVAGTEPGRHPQA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytosol {ECO:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11495</th>\n",
       "      <td>Q8NBC4</td>\n",
       "      <td>MFPRPVLNSRAQAILLPQPPNMLDHRQWPPRLASFPFTKTGMLSRA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11535</th>\n",
       "      <td>Q8TDY3</td>\n",
       "      <td>MFNPHALDSPAVIFDNGSGFCKAGLSGEFGPRHMVSSIVGHLKFQA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547</th>\n",
       "      <td>Q8WWF8</td>\n",
       "      <td>MAGTARHDREMAIQAKKKLTTATDPIERLRLQCLARGSAGIKGLGR...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11671</th>\n",
       "      <td>Q9NUJ7</td>\n",
       "      <td>MGGQVSASNSFSRLHCRNANEDWMSALCPRLWDVPLHHLSIPGSHD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11679</th>\n",
       "      <td>Q9P2W6</td>\n",
       "      <td>MGRTWCGMWRRRRPGRRSAVPRWPHLSSQSGVEPPDRWTGTPGWPS...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2644 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry                                           Sequence  \\\n",
       "9      A1E959  MKIIILLGFLGATLSAPLIPQRLMSASNSNELLLNLNNGQLLPLQL...   \n",
       "14     A1XBS5  MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...   \n",
       "18     A2RU49  MSSGNYQQSEALSKPTFSEEQASALVESVFGLKVSKVRPLPSYDDQ...   \n",
       "20     A2RUH7  MEAATAPEVAAGSKLKVKEASPADAEPPQASPGQGAGSPTPQLLPP...   \n",
       "21     A4D126  MEAGPPGSARPAEPGPCLSGQRGADHTASASLQSVAGTEPGRHPQA...   \n",
       "...       ...                                                ...   \n",
       "11495  Q8NBC4  MFPRPVLNSRAQAILLPQPPNMLDHRQWPPRLASFPFTKTGMLSRA...   \n",
       "11535  Q8TDY3  MFNPHALDSPAVIFDNGSGFCKAGLSGEFGPRHMVSSIVGHLKFQA...   \n",
       "11547  Q8WWF8  MAGTARHDREMAIQAKKKLTTATDPIERLRLQCLARGSAGIKGLGR...   \n",
       "11671  Q9NUJ7  MGGQVSASNSFSRLHCRNANEDWMSALCPRLWDVPLHHLSIPGSHD...   \n",
       "11679  Q9P2W6  MGRTWCGMWRRRRPGRRSAVPRWPHLSSQSGVEPPDRWTGTPGWPS...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "9      SUBCELLULAR LOCATION: Secreted {ECO:0000250|Un...  \n",
       "14     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "18        SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.  \n",
       "20     SUBCELLULAR LOCATION: Cytoplasm, myofibril, sa...  \n",
       "21     SUBCELLULAR LOCATION: Cytoplasm, cytosol {ECO:...  \n",
       "...                                                  ...  \n",
       "11495  SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "11535  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton ...  \n",
       "11547     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.  \n",
       "11671  SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "11679                   SUBCELLULAR LOCATION: Cytoplasm.  \n",
       "\n",
       "[2644 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()  # Drop proteins with missing columns\n",
    "cytosolic = df['Subcellular location [CC]'].str.contains(\"Cytosol\") | df['Subcellular location [CC]'].str.contains(\"Cytoplasm\")\n",
    "membrane = df['Subcellular location [CC]'].str.contains(\"Membrane\") | df['Subcellular location [CC]'].str.contains(\"Cell membrane\")\n",
    "cytosolic_df = df[cytosolic & ~membrane]\n",
    "cytosolic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7079889f-4942-420b-82cc-392b3de0f420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6</td>\n",
       "      <td>GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A2RU14</td>\n",
       "      <td>MAGTVLGVGAGVFILALLWVAVLLLCVLLSRASGAARFSVIFLFFG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A5X5Y0</td>\n",
       "      <td>MEGSWFHRKRFSFYLLLGFLLQGRGVTFTINCSGFGQHGADPTALN...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Postsynaptic cell membra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>A6ND01</td>\n",
       "      <td>MACWWPLLLELWTVMPTWAGDELLNICMNAKHHKRVPSPEDKLYEE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cell membrane {ECO:00002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11895</th>\n",
       "      <td>Q86UQ5</td>\n",
       "      <td>MQSDIYHPGHSFPSWVLCWVHSCGHEGHLRETAEIRKTHQNGDLQI...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11918</th>\n",
       "      <td>Q8N8V8</td>\n",
       "      <td>MLLKVRRASLKPPATPHQGAFRAGNVIGQLIYLLTWSLFTAWLRPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11955</th>\n",
       "      <td>Q96N68</td>\n",
       "      <td>MQGQGALKESHIHLPTEQPEASLVLQGQLAESSALGPKGALRPQAQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11963</th>\n",
       "      <td>Q9H0A3</td>\n",
       "      <td>MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVT...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11966</th>\n",
       "      <td>Q9H354</td>\n",
       "      <td>MNKHNLRLVQLASELILIEIIPKLFLSQVTTISHIKREKIPPNHRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2538 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "3          A0M8Q6  GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...   \n",
       "17         A2RU14  MAGTVLGVGAGVFILALLWVAVLLLCVLLSRASGAARFSVIFLFFG...   \n",
       "33         A5X5Y0  MEGSWFHRKRFSFYLLLGFLLQGRGVTFTINCSGFGQHGADPTALN...   \n",
       "36         A6ND01  MACWWPLLLELWTVMPTWAGDELLNICMNAKHHKRVPSPEDKLYEE...   \n",
       "...           ...                                                ...   \n",
       "11895      Q86UQ5  MQSDIYHPGHSFPSWVLCWVHSCGHEGHLRETAEIRKTHQNGDLQI...   \n",
       "11918      Q8N8V8  MLLKVRRASLKPPATPHQGAFRAGNVIGQLIYLLTWSLFTAWLRPP...   \n",
       "11955      Q96N68  MQGQGALKESHIHLPTEQPEASLVLQGQLAESSALGPKGALRPQAQ...   \n",
       "11963      Q9H0A3  MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVT...   \n",
       "11966      Q9H354  MNKHNLRLVQLASELILIEIIPKLFLSQVTTISHIKREKIPPNHRK...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "0      SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...  \n",
       "3      SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...  \n",
       "17     SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "33     SUBCELLULAR LOCATION: Postsynaptic cell membra...  \n",
       "36     SUBCELLULAR LOCATION: Cell membrane {ECO:00002...  \n",
       "...                                                  ...  \n",
       "11895  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11918  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11955  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11963  SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...  \n",
       "11966  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "\n",
       "[2538 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "membrane_df = df[membrane & ~cytosolic]\n",
    "membrane_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c00518a4-9750-47cf-aac6-f8789028a72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cytosolic_sequences = cytosolic_df[\"Sequence\"].tolist()\n",
    "cytosolic_labels = [0 for protein in cytosolic_sequences]\n",
    "membrane_sequences = membrane_df[\"Sequence\"].tolist()\n",
    "membrane_labels = [1 for protein in membrane_sequences]\n",
    "\n",
    "sequences = cytosolic_sequences + membrane_sequences\n",
    "labels = cytosolic_labels + membrane_labels\n",
    "\n",
    "# Quick check to make sure we got it right\n",
    "len(sequences) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9bdc9c-50eb-402b-97a5-3f4a7af4a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6b7b2f-fdb4-483f-bfdd-e30cebdb2ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2025-02-16 05:05:10 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import Callback, RichModelSummary\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from megatron.core.optimizer.optimizer_config import OptimizerConfig\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm as nllm\n",
    "from nemo.lightning import resume\n",
    "from nemo.lightning.nemo_logger import NeMoLogger\n",
    "from nemo.lightning.pytorch import callbacks as nl_callbacks\n",
    "from nemo.lightning.pytorch.callbacks.model_transform import ModelTransform\n",
    "from nemo.lightning.pytorch.callbacks.peft import PEFT\n",
    "from nemo.lightning.pytorch.optim.megatron import MegatronOptimizerModule\n",
    "\n",
    "from bionemo.core.data.load import load\n",
    "from bionemo.esm2.api import ESM2GenericConfig\n",
    "from bionemo.esm2.data.tokenizer import BioNeMoESMTokenizer, get_tokenizer\n",
    "from bionemo.esm2.model.finetune.datamodule import ESM2FineTuneDataModule\n",
    "from bionemo.esm2.model.finetune.finetune_regressor import ESM2FineTuneSeqConfig, InMemorySingleValueDataset\n",
    "from bionemo.llm.model.biobert.lightning import biobert_lightning_module\n",
    "\n",
    "__all__: Sequence[str] = (\"train_model\",)\n",
    "\n",
    "def train_model(\n",
    "    experiment_name: str,\n",
    "    experiment_dir: Path,\n",
    "    config: ESM2GenericConfig,\n",
    "    data_module: pl.LightningDataModule,\n",
    "    n_steps_train: int,\n",
    "    metric_tracker: Callback | None = None,\n",
    "    tokenizer: BioNeMoESMTokenizer = get_tokenizer(),\n",
    "    peft: PEFT | None = None,\n",
    "    _use_rich_model_summary: bool = True,\n",
    ") -> Tuple[Path, Callback | None, nl.Trainer]:\n",
    "    \"\"\"Trains a BioNeMo ESM2 model using PyTorch Lightning.\n",
    "\n",
    "    Parameters:\n",
    "        experiment_name: The name of the experiment.\n",
    "        experiment_dir: The directory where the experiment will be saved.\n",
    "        config: The configuration for the ESM2 model.\n",
    "        data_module: The data module for training and validation.\n",
    "        n_steps_train: The number of training steps.\n",
    "        metric_tracker: Optional callback to track metrics\n",
    "        tokenizer: The tokenizer to use. Defaults to `get_tokenizer()`.\n",
    "        peft: The PEFT (Parameter-Efficient Fine-Tuning) module. Defaults to None.\n",
    "        _use_rich_model_summary: Whether to use the RichModelSummary callback, omitted in our test suite until\n",
    "            https://nvbugspro.nvidia.com/bug/4959776 is resolved. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the path to the saved checkpoint, a MetricTracker\n",
    "        object, and the PyTorch Lightning Trainer object.\n",
    "    \"\"\"\n",
    "    checkpoint_callback = nl_callbacks.ModelCheckpoint(\n",
    "        save_last=True,\n",
    "        save_on_train_epoch_end=True,\n",
    "        monitor=\"reduced_train_loss\",  # TODO find out how to get val_loss logged and use \"val_loss\",\n",
    "        every_n_train_steps=n_steps_train // 2,\n",
    "        always_save_context=True,  # Enables the .nemo file-like checkpointing where all IOMixins are under SerDe\n",
    "    )\n",
    "\n",
    "    # Setup the logger and train the model\n",
    "    nemo_logger = NeMoLogger(\n",
    "        log_dir=str(experiment_dir),\n",
    "        name=experiment_name,\n",
    "        tensorboard=TensorBoardLogger(save_dir=experiment_dir, name=experiment_name),\n",
    "        ckpt=checkpoint_callback,\n",
    "    )\n",
    "        # Needed so that the trainer can find an output directory for the profiler\n",
    "    # ckpt_path needs to be a string for SerDe\n",
    "    optimizer = MegatronOptimizerModule(\n",
    "        config=OptimizerConfig(\n",
    "            lr=5e-4,\n",
    "            optimizer=\"adam\",\n",
    "            use_distributed_optimizer=True,\n",
    "            fp16=config.fp16,\n",
    "            bf16=config.bf16,\n",
    "        )\n",
    "    )\n",
    "    module = biobert_lightning_module(config=config, tokenizer=tokenizer, optimizer=optimizer, model_transform=peft)\n",
    "\n",
    "    strategy = nl.MegatronStrategy(\n",
    "        tensor_model_parallel_size=1,\n",
    "        pipeline_model_parallel_size=1,\n",
    "        ddp=\"megatron\",\n",
    "        find_unused_parameters=True,\n",
    "        enable_nemo_ckpt_io=True,\n",
    "    )\n",
    "    strategy = 'ddp_notebook'\n",
    "\n",
    "    if _use_rich_model_summary:\n",
    "        # RichModelSummary is not used in the test suite until https://nvbugspro.nvidia.com/bug/4959776 is resolved due\n",
    "        # to errors with serialization / deserialization.\n",
    "        callbacks: list[Callback] = [RichModelSummary(max_depth=4)]\n",
    "    else:\n",
    "        callbacks = []\n",
    "\n",
    "    if metric_tracker is not None:\n",
    "        callbacks.append(metric_tracker)\n",
    "    if peft is not None:\n",
    "        callbacks.append(\n",
    "            ModelTransform()\n",
    "        )  # Callback needed for PEFT fine-tuning using NeMo2, i.e. biobert_lightning_module(model_transform=peft).\n",
    "    trainer = nl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        strategy=strategy,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=n_steps_train // 2,\n",
    "        max_steps=n_steps_train,\n",
    "        num_nodes=1,\n",
    "        log_every_n_steps=n_steps_train // 2,\n",
    "        callbacks=callbacks,\n",
    "        plugins=nl.MegatronMixedPrecision(precision=\"bf16-mixed\"),\n",
    "    )\n",
    "    nllm.train(\n",
    "        model=module,\n",
    "        data=data_module,\n",
    "        trainer=trainer,\n",
    "        log=nemo_logger,\n",
    "        resume=resume.AutoResume(\n",
    "            resume_if_exists=True,  # Looks for the -last checkpoint to continue training.\n",
    "            resume_ignore_no_checkpoint=True,  # When false this will throw an error with no existing checkpoint.\n",
    "        ),\n",
    "    )\n",
    "    ckpt_path = Path(checkpoint_callback.last_model_path.replace(\".ckpt\", \"\"))\n",
    "    return ckpt_path, metric_tracker, trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144c40fe-c963-4e81-80b0-4e5e330b92d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2025-02-16 05:05:17 nemo_logging:405] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-02-16 05:05:17 nemo_logging:393] Experiments will be logged at /tmp/tmpk_t1tv55/finetune_regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-02-16 05:05:17 nemo_logging:405] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /tmp/tmpk_t1tv55/../../../tmp/tmpk_t1tv55\n",
      "[NeMo W 2025-02-16 05:05:17 nemo_logging:405] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "[NeMo W 2025-02-16 05:05:17 nemo_logging:405] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/tmp/tmpk_t1tv55/finetune_regressor/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m pretrain_ckpt_path \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesm2/650m:2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m config \u001b[38;5;241m=\u001b[39m ESM2FineTuneSeqConfig(initial_ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(pretrain_ckpt_path))\n\u001b[0;32m---> 18\u001b[0m checkpoint, metrics, trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_results_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# new checkpoint will land in a subdir of this\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# same config as before since we are just continuing training\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment completed with checkpoint stored at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 118\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(experiment_name, experiment_dir, config, data_module, n_steps_train, metric_tracker, tokenizer, peft, _use_rich_model_summary)\u001b[0m\n\u001b[1;32m    103\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    104\u001b[0m         ModelTransform()\n\u001b[1;32m    105\u001b[0m     )  \u001b[38;5;66;03m# Callback needed for PEFT fine-tuning using NeMo2, i.e. biobert_lightning_module(model_transform=peft).\u001b[39;00m\n\u001b[1;32m    106\u001b[0m trainer \u001b[38;5;241m=\u001b[39m nl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    107\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    108\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     plugins\u001b[38;5;241m=\u001b[39mnl\u001b[38;5;241m.\u001b[39mMegatronMixedPrecision(precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbf16-mixed\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 118\u001b[0m \u001b[43mnllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnemo_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoResume\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_if_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Looks for the -last checkpoint to continue training.\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_ignore_no_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# When false this will throw an error with no existing checkpoint.\u001b[39;49;00m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m Path(checkpoint_callback\u001b[38;5;241m.\u001b[39mlast_model_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ckpt_path, metric_tracker, trainer\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/api.py:106\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, trainer, log, resume, optim, tokenizer, model_transform)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mTrains a model using the specified data and trainer, with optional tokenizer, source, and export.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    PosixPath('/path/to/log_dir')\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m app_state \u001b[38;5;241m=\u001b[39m _setup(\n\u001b[1;32m     96\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     97\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     model_transform\u001b[38;5;241m=\u001b[39mmodel_transform,\n\u001b[1;32m    104\u001b[0m )\n\u001b[0;32m--> 106\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m app_state\u001b[38;5;241m.\u001b[39mexp_dir\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/trainer/call.py:46\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/pytorch/strategies/launchers/multiprocessing.py:110\u001b[0m, in \u001b[0;36m_MultiProcessingLauncher.launch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Launches processes that run the given function in parallel.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03mThe function is allowed to have a return value. However, when all processes join, only the return value\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforkserver\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 110\u001b[0m     \u001b[43m_check_bad_cuda_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    112\u001b[0m     _check_missing_main_guard()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/lightning/fabric/strategies/launchers/multiprocessing.py:208\u001b[0m, in \u001b[0;36m_check_bad_cuda_fork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _IS_INTERACTIVE:\n\u001b[1;32m    207\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You will have to restart the Python kernel.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Lightning can't create new processes if CUDA is already initialized. Did you manually call `torch.cuda.*` functions, have moved the model to the device, or allocated memory on the GPU any other way? Please remove any such calls, or change the selected strategy. You will have to restart the Python kernel."
     ]
    }
   ],
   "source": [
    "# set the results directory\n",
    "experiment_results_dir = tempfile.TemporaryDirectory().name\n",
    "\n",
    "train_dataset = InMemorySingleValueDataset(list(zip(train_sequences, train_labels)))\n",
    "test_dataset = InMemorySingleValueDataset(list(zip(test_sequences, test_labels)))\n",
    "\n",
    "data_module = ESM2FineTuneDataModule(train_dataset=train_dataset, valid_dataset=test_dataset)\n",
    "\n",
    "experiment_name = \"finetune_regressor\"\n",
    "n_steps_train = 50\n",
    "seed = 42\n",
    "\n",
    "# To download a 650M pre-trained ESM2 model\n",
    "pretrain_ckpt_path = load(\"esm2/650m:2.0\")\n",
    "\n",
    "config = ESM2FineTuneSeqConfig(initial_ckpt_path=str(pretrain_ckpt_path))\n",
    "\n",
    "checkpoint, metrics, trainer = train_model(\n",
    "    experiment_name=experiment_name,\n",
    "    experiment_dir=Path(experiment_results_dir),  # new checkpoint will land in a subdir of this\n",
    "    config=config,  # same config as before since we are just continuing training\n",
    "    data_module=data_module,\n",
    "    n_steps_train=n_steps_train,\n",
    ")\n",
    "print(f\"Experiment completed with checkpoint stored at {checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee349a80-56ad-49bc-870e-2af19396d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nl.Trainer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
