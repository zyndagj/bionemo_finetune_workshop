{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6441711c-a040-4895-8500-b2af0cc6909c",
   "metadata": {},
   "source": [
    "# Fine-tuning ESM-2\n",
    "\n",
    "In this notebook, we'll be fine-tuning ESM-2 to predict the subcelluar location of proteins based on the input amino acid sequence. We'll start by showing how this can be done with the published model [hosted on HuggingFace](https://huggingface.co/facebook/esm2_t33_650M_UR50D) and then showing how this can be done using NVIDIA's [BioNeMo 2 Framework](https://docs.nvidia.com/bionemo-framework/latest/user-guide/)\n",
    "\n",
    "> Inspired by ESM-2's [example notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) for fine-tuning.\n",
    "\n",
    "This notebook needs the [evaluate](https://pypi.org/project/evaluate/) package, but it's not present in the BioNeMo v2.3 container, so we'll need to manually install it with `pip`\n",
    "\n",
    "> Make sure to restart the kernel of this notebook after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7183b2-be3f-4fa9-b227-909c2d63aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "# Restart kernel after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import requests, pandas, os, evaluate\n",
    "# Rerun previous cell and restart kernel if this fails\n",
    "from io import BytesIO\n",
    "\n",
    "# Set environment variables for huggingface\n",
    "for var in ['HF_HOME','HF_HUB_CACHE']:\n",
    "    os.environ[var] = '/tmp/hf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee575bec",
   "metadata": {},
   "source": [
    "We're going to fine-tune ESM2-650M using human protein sequences (`organism_id:9606`) that we reviewed (`reviewed:true`), and range from 80 to 500 amino acids in length (`length:[80 TO 500]`) and only outputting the `Sequence` and `Subcellular location [CC]` columns. [UniProt](https://www.uniprot.org/) actually has a REST API, so this query has been encoded into `query_url`.\n",
    "\n",
    "This download can sometimes fail or take a while, so we cache the data in parquet format once it succeeds. This will make any repeated runs of the notebook go much faster on the same compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5549b6-acee-4c42-82b7-0560ff59b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_url = \"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Ccc_subcellular_location&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B80%20TO%20500%5D%29%29\"\n",
    "tmp_file = \"/tmp/uniprot.parquet.gz\"\n",
    "\n",
    "# Logic to quickly load data if cached\n",
    "if not os.path.exists(tmp_file):\n",
    "    # Download data\n",
    "    uniprot_request = requests.get(query_url)\n",
    "    # Store data as binary object that works like a file\n",
    "    bio = BytesIO(uniprot_request.content)\n",
    "    # Read binary object as compressed csv\n",
    "    df = pandas.read_csv(bio, compression='gzip', sep='\\t')\n",
    "    # Cache to local location for faster reloads\n",
    "    df.to_parquet(tmp_file, compression=\"gzip\")\n",
    "else:\n",
    "    # Load from cache\n",
    "    df = pandas.read_parquet(tmp_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d0f04",
   "metadata": {},
   "source": [
    "Our goal is to train a model that can predict proteins that are located in the [cytosol](https://en.wikipedia.org/wiki/Cytosol) (intracellular fluid) or the membrane of a cell. Proteins in the cytosol have a `Subcellular location [CC]` of `Cytoplasm` or `Cytosol`. Proteins in the cell membrane have a `Subcellular location [CC]` of `Membrane` or `Cell membrane`.\n",
    "\n",
    "Once these are selected, we can create new dataframes for each type, while also excluding proteins that exist in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4ae7c9-4161-40c9-850d-6d5036f022fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop proteins with missing columns\n",
    "df = df.dropna()\n",
    "# Get ids of proteins with Cytosol or Cytoplasm locations\n",
    "cytosolic = df['Subcellular location [CC]'].str.contains(\"Cytosol\") | df['Subcellular location [CC]'].str.contains(\"Cytoplasm\")\n",
    "# Get ids of proteins with Membrane or Cell membrane locations\n",
    "membrane = df['Subcellular location [CC]'].str.contains(\"Membrane\") | df['Subcellular location [CC]'].str.contains(\"Cell membrane\")\n",
    "\n",
    "# Create new cytosolic dataframe with proteins\n",
    "cytosolic_df = df[cytosolic & ~membrane]\n",
    "cytosolic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7079889f-4942-420b-82cc-392b3de0f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new membrane dataframe with proteins\n",
    "membrane_df = df[membrane & ~cytosolic]\n",
    "membrane_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1677d52",
   "metadata": {},
   "source": [
    "Now that we've filtered and separated out he proteins of interest, we can extract the sequences, and encode the locations as `0` for cytosolic proteins and `1` for membrane proteins. Then, we can combine the types together into two lists: sequences and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00518a4-9750-47cf-aac6-f8789028a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cytosolic_sequences = cytosolic_df[\"Sequence\"].tolist()\n",
    "cytosolic_labels = [0 for protein in cytosolic_sequences]\n",
    "membrane_sequences = membrane_df[\"Sequence\"].tolist()\n",
    "membrane_labels = [1 for protein in membrane_sequences]\n",
    "\n",
    "sequences = cytosolic_sequences + membrane_sequences\n",
    "labels = cytosolic_labels + membrane_labels\n",
    "\n",
    "# Quick check to make sure we got it right\n",
    "assert(len(sequences) == len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff606e",
   "metadata": {},
   "source": [
    "When training a model, you need training data for the model to learn from and validation data that the model never learns from to make sure what it learns is generally applicable. We're going to use sklearn to split our lists 75% and 25% into `train` and `test` datasets. Once that is done, we'll cache the datasets to CSV (comma separate values) format so both fine-tuning methods use the same data.\n",
    "\n",
    "BioNeMo also only validates for two steps while training, so we're going to truncate our test data to 64 items (8 batches of 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9bdc9c-50eb-402b-97a5-3f4a7af4a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)\n",
    "\n",
    "pandas.DataFrame({\"sequences\":train_sequences, \"labels\":train_labels}).to_csv(\"/tmp/train_df.csv\")\n",
    "pandas.DataFrame({\"sequences\":test_sequences, \"labels\":test_labels}).to_csv(\"/tmp/test_df.csv\")\n",
    "\n",
    "# Only keep the first 16 validation sequences for parity with BioNeMo\n",
    "test_sequences_small = test_sequences[:64]\n",
    "test_labels_small = test_labels[:64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f18291",
   "metadata": {},
   "source": [
    "Inputs sequences need to be tokenized into numerical format for the model. When we pull down the ESM2-650M model checkpoint from HuggingFace, we get the tokenizer too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae2a58-6801-4228-8536-b1b35440cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Using the ESM2 650M model checkpoint from HuggingFace\n",
    "model_checkpoint = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "# Load the tokenizer from the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff79cf6",
   "metadata": {},
   "source": [
    "To understand what happens during tokenization, we can tokenize the first sequence.\n",
    "\n",
    "You'll notice that the tokenized sequence is 2 values longer than the original sequence. That's because there are tokens to represent the START and END of the sequence for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df582064-246f-4343-a1a6-b77dedb0304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the first sequence for demonstration\n",
    "seq = train_sequences[0]\n",
    "tokenized = tokenizer(seq)\n",
    "\n",
    "print(f\"Sequence length: {len(seq)}\\nToken length: {len(tokenized['input_ids'])}\")\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2475b80-405a-4e88-927b-b0c79131fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all sequences\n",
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a4e9b",
   "metadata": {},
   "source": [
    "After tokenizing all of our sequences, we can create a `Dataset` object that will handle data shuffling and iterating while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348deb2a-6cce-4e46-a537-4801974ee75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "# Add labels to Dataset\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels_small)\n",
    "\n",
    "# Print the shape and columns of the datasets after adding labels\n",
    "print(train_dataset.shape, list(train_dataset[0].keys()))\n",
    "print(test_dataset.shape, list(test_dataset[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7695ad",
   "metadata": {},
   "source": [
    "Now that data preparation is done, we can pull the model and configure it. We're going to be training the model to classify an input sequence as one of two labels {0: cytosolic, 1: membrane}, so we're going to load the ESM-2 650M checkpoint using the `AutoModelForSequenceClassification` class. Notice that we're also telling it that there are two possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f92ae8-e17b-444f-83d7-b8921e0c8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = max(train_labels + test_labels) + 1  # 2: {0: cytosolic, 1: membrane}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df3d84",
   "metadata": {},
   "source": [
    "After loading the model, we can configure the training run. To mimic the way BioNeMo does fine-tuning, we're going to train for 200 steps and evaluate every 50 steps. On a 48GB GPU like the L40S, a batch size of 8 can be used. For other GPUs, experiment with other values based on memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0dcbf-12b8-48b3-b900-dd6d4dece154",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 8 # Works for 48GB GPU\n",
    "strat = \"steps\" # \"epoch\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"/tmp/{model_name}-finetuned-localization\", # Make sure to change this for a real model\n",
    "    eval_strategy = strat,\n",
    "    save_strategy = strat,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    max_steps=200,\n",
    "    eval_steps=50,\n",
    "    #num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    include_tokens_per_second=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa491977",
   "metadata": {},
   "source": [
    "We're also going to use HuggingFace's evaluate package to compute the accuracy of the classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820334ca-af52-43c1-8b07-a1b3ee5c0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226002a",
   "metadata": {},
   "source": [
    "Up next, we create the `Trainer` class using the model, data, and configurations. If everything is valid, we can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b65d8f-0930-443f-9947-99a00eb83ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e795d1c-86d7-4968-b4d5-d58ff4739c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dac9f0-8b55-45e4-93a6-bff8283dd292",
   "metadata": {},
   "source": [
    "Once training is done, stop this notebook so the model is freed from memory and we can do fine-tuning with BioNeMo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
